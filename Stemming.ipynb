{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yem8wOEvGAKy"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.stem.porter import * #porter is the algorithm that splitting it\n",
        "stemmer = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['run','runner','running','ran','runs','easily','fairly']\n",
        "for word in words:\n",
        "    print(word+' ----> '+stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWrRUcZQIBwH",
        "outputId": "580e2d8d-ff64-4c2e-b692-f7aceb4e2310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run ----> run\n",
            "runner ----> runner\n",
            "running ----> run\n",
            "ran ----> ran\n",
            "runs ----> run\n",
            "easily ----> easili\n",
            "fairly ----> fairli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "s_stemmer = SnowballStemmer(language='english')\n",
        "\n",
        "words = ['run','runner','running','ran','runs','easily','fairly']\n",
        "for word in words:\n",
        "  print(word+' ----> '+s_stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnUphbTCInSG",
        "outputId": "363f9c30-d427-42e1-a155-dee3f0212682"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run ----> run\n",
            "runner ----> runner\n",
            "running ----> run\n",
            "ran ----> ran\n",
            "runs ----> run\n",
            "easily ----> easili\n",
            "fairly ----> fair\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['generous','generation','generously','generate']\n",
        "for word in words:\n",
        "  print(word+' ----> '+s_stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvsw85XRJQWR",
        "outputId": "72296dcd-b396-4146-ee66-8d704d0c4694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generous ----> generous\n",
            "generation ----> generat\n",
            "generously ----> generous\n",
            "generate ----> generat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, LancasterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "ps = PorterStemmer()\n",
        "ls = LancasterStemmer() # is better\n",
        "words = words = ['run','runner','running','ran','runs','easily','fairly']\n",
        "for w in words:\n",
        "    print(w, \" : \", ps.stem(w))\n",
        "    print(w, \" : \", ls.stem(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9A2g4nlyJVH9",
        "outputId": "9fd6ad2b-6db9-488c-e45f-f079a6d0937a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run  :  run\n",
            "run  :  run\n",
            "runner  :  runner\n",
            "runner  :  run\n",
            "running  :  run\n",
            "running  :  run\n",
            "ran  :  ran\n",
            "ran  :  ran\n",
            "runs  :  run\n",
            "runs  :  run\n",
            "easily  :  easili\n",
            "easily  :  easy\n",
            "fairly  :  fairli\n",
            "fairly  :  fair\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['is', 'was', 'be', 'been', 'are', 'were', 'being']\n",
        "for w in words:\n",
        "    print(w, \" : \", ps.stem(w))\n",
        "    print(w, \" : \", ls.stem(w))\n",
        "    print('------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwthKbCdKl2b",
        "outputId": "02ee96c7-d0b6-446d-ea45-1522080e38d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "is  :  is\n",
            "is  :  is\n",
            "------\n",
            "was  :  wa\n",
            "was  :  was\n",
            "------\n",
            "be  :  be\n",
            "be  :  be\n",
            "------\n",
            "been  :  been\n",
            "been  :  been\n",
            "------\n",
            "are  :  are\n",
            "are  :  ar\n",
            "------\n",
            "were  :  were\n",
            "were  :  wer\n",
            "------\n",
            "being  :  be\n",
            "being  :  being\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['book','booking', 'booked','books', 'booker', 'bookstar']\n",
        "\n",
        "for w in words:\n",
        "    print(w, \" : \", ps.stem(w))\n",
        "    print(w, \" : \", ls.stem(w))\n",
        "    print('------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A58bLsgjK9O7",
        "outputId": "409ff497-d87e-4c13-d439-1c5daadba4f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "book  :  book\n",
            "book  :  book\n",
            "------\n",
            "booking  :  book\n",
            "booking  :  book\n",
            "------\n",
            "booked  :  book\n",
            "booked  :  book\n",
            "------\n",
            "books  :  book\n",
            "books  :  book\n",
            "------\n",
            "booker  :  booker\n",
            "booker  :  book\n",
            "------\n",
            "bookstar  :  bookstar\n",
            "bookstar  :  bookst\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2DpIeqDMIy1",
        "outputId": "c2d80656-3b09-472f-820f-cec386e3b2c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "had  :  had\n",
            "had  :  had\n",
            "------\n",
            "you  :  you\n",
            "you  :  you\n",
            "------\n",
            "booked  :  book\n",
            "booked  :  book\n",
            "------\n",
            "the  :  the\n",
            "the  :  the\n",
            "------\n",
            "air  :  air\n",
            "air  :  air\n",
            "------\n",
            "booking  :  book\n",
            "booking  :  book\n",
            "------\n",
            "already  :  alreadi\n",
            "already  :  already\n",
            "------\n",
            "?  :  ?\n",
            "?  :  ?\n",
            "------\n",
            "if  :  if\n",
            "if  :  if\n",
            "------\n",
            "not  :  not\n",
            "not  :  not\n",
            "------\n",
            "try  :  tri\n",
            "try  :  try\n",
            "------\n",
            "to  :  to\n",
            "to  :  to\n",
            "------\n",
            "book  :  book\n",
            "book  :  book\n",
            "------\n",
            "is  :  is\n",
            "is  :  is\n",
            "------\n",
            "ASAP  :  asap\n",
            "ASAP  :  asap\n",
            "------\n",
            "since  :  sinc\n",
            "since  :  sint\n",
            "------\n",
            "booking  :  book\n",
            "booking  :  book\n",
            "------\n",
            "will  :  will\n",
            "will  :  wil\n",
            "------\n",
            "be  :  be\n",
            "be  :  be\n",
            "------\n",
            "out  :  out\n",
            "out  :  out\n",
            "------\n",
            "of  :  of\n",
            "of  :  of\n",
            "------\n",
            "books  :  book\n",
            "books  :  book\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'had you booked the air booking already? if not try to book is ASAP since booking will be out of books'\n",
        "words = word_tokenize(sentence)\n",
        "for w in words:\n",
        "    print(w, \" : \", ps.stem(w))\n",
        "    print(w, \" : \", s_stemmer.stem(w))\n",
        "    print(w, \" : \", ls.stem(w))\n",
        "    print('------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diwxUe8NNZWZ",
        "outputId": "3af82cd1-64e7-456e-c213-bf69f8869cc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "had  :  had\n",
            "had  :  had\n",
            "had  :  had\n",
            "------\n",
            "you  :  you\n",
            "you  :  you\n",
            "you  :  you\n",
            "------\n",
            "booked  :  book\n",
            "booked  :  book\n",
            "booked  :  book\n",
            "------\n",
            "the  :  the\n",
            "the  :  the\n",
            "the  :  the\n",
            "------\n",
            "air  :  air\n",
            "air  :  air\n",
            "air  :  air\n",
            "------\n",
            "booking  :  book\n",
            "booking  :  book\n",
            "booking  :  book\n",
            "------\n",
            "already  :  alreadi\n",
            "already  :  alreadi\n",
            "already  :  already\n",
            "------\n",
            "?  :  ?\n",
            "?  :  ?\n",
            "?  :  ?\n",
            "------\n",
            "if  :  if\n",
            "if  :  if\n",
            "if  :  if\n",
            "------\n",
            "not  :  not\n",
            "not  :  not\n",
            "not  :  not\n",
            "------\n",
            "try  :  tri\n",
            "try  :  tri\n",
            "try  :  try\n",
            "------\n",
            "to  :  to\n",
            "to  :  to\n",
            "to  :  to\n",
            "------\n",
            "book  :  book\n",
            "book  :  book\n",
            "book  :  book\n",
            "------\n",
            "is  :  is\n",
            "is  :  is\n",
            "is  :  is\n",
            "------\n",
            "ASAP  :  asap\n",
            "ASAP  :  asap\n",
            "ASAP  :  asap\n",
            "------\n",
            "since  :  sinc\n",
            "since  :  sinc\n",
            "since  :  sint\n",
            "------\n",
            "booking  :  book\n",
            "booking  :  book\n",
            "booking  :  book\n",
            "------\n",
            "will  :  will\n",
            "will  :  will\n",
            "will  :  wil\n",
            "------\n",
            "be  :  be\n",
            "be  :  be\n",
            "be  :  be\n",
            "------\n",
            "out  :  out\n",
            "out  :  out\n",
            "out  :  out\n",
            "------\n",
            "of  :  of\n",
            "of  :  of\n",
            "of  :  of\n",
            "------\n",
            "books  :  book\n",
            "books  :  book\n",
            "books  :  book\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
        "print(\"{0:20}{1:20}{2:20}{3:20}\".format(\"Word\",\"Porter Stemmer\",\"lancaster Stemmer\",'Snowball stemmer'))\n",
        "for word in word_list:\n",
        "    print(\"{0:20}{1:20}{2:20}{3:20}\".format(word,ps.stem(word),ls.stem(word),s_stemmer.stem(word)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfHiFGqeNcOC",
        "outputId": "d0b24146-b6d0-4ae0-f855-e46ac63335b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word                Porter Stemmer      lancaster Stemmer   Snowball stemmer    \n",
            "friend              friend              friend              friend              \n",
            "friendship          friendship          friend              friendship          \n",
            "friends             friend              friend              friend              \n",
            "friendships         friendship          friend              friendship          \n",
            "stabil              stabil              stabl               stabil              \n",
            "destabilize         destabil            dest                destabil            \n",
            "misunderstanding    misunderstand       misunderstand       misunderstand       \n",
            "railroad            railroad            railroad            railroad            \n",
            "moonlight           moonlight           moonlight           moonlight           \n",
            "football            footbal             footbal             footbal             \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(\"i am a runner running in a race because i love to run since i ran yesterday.\")\n",
        "for token in doc:\n",
        "    print(token.text, '\\t', token.pos_, '\\t', token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoYFpJz-PBQs",
        "outputId": "56b99de2-9a7f-457d-c750-a110d15564c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i \t PRON \t I\n",
            "am \t AUX \t be\n",
            "a \t DET \t a\n",
            "runner \t NOUN \t runner\n",
            "running \t VERB \t run\n",
            "in \t ADP \t in\n",
            "a \t DET \t a\n",
            "race \t NOUN \t race\n",
            "because \t SCONJ \t because\n",
            "i \t PRON \t I\n",
            "love \t VERB \t love\n",
            "to \t PART \t to\n",
            "run \t VERB \t run\n",
            "since \t SCONJ \t since\n",
            "i \t PRON \t I\n",
            "ran \t VERB \t run\n",
            "yesterday \t NOUN \t yesterday\n",
            ". \t PUNCT \t .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_lemmas(text):\n",
        "    for token in text:\n",
        "        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}')"
      ],
      "metadata": {
        "id": "edLSnCteQ1Ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc2 = nlp(u\"I saw eighteen mice today!\")\n",
        "show_lemmas(doc2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2UPViHUQ_hA",
        "outputId": "1609a6e2-5c9d-4e33-b67f-ce1db1a2134b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I            PRON   4690420944186131903    I\n",
            "saw          VERB   11925638236994514241   see\n",
            "eighteen     NUM    9609336664675087640    eighteen\n",
            "mice         NOUN   1384165645700560590    mouse\n",
            "today        NOUN   11042482332948150395   today\n",
            "!            PUNCT  17494803046312582752   !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc3 = nlp(u\"I am meeting him right now at the meeting.\")\n",
        "show_lemmas(doc3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxyFnRxNRQAs",
        "outputId": "fdfe4fdc-e762-4048-eb87-d4f17be78366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I            PRON   4690420944186131903    I\n",
            "am           AUX    10382539506755952630   be\n",
            "meeting      VERB   6880656908171229526    meet\n",
            "him          PRON   1655312771067108281    he\n",
            "right        ADV    5943797630011647483    right\n",
            "now          ADV    17157488710739566268   now\n",
            "at           ADP    11667289587015813222   at\n",
            "the          DET    7425985699627899538    the\n",
            "meeting      NOUN   14798207169164081740   meeting\n",
            ".            PUNCT  12646065887601541794   .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lf3VUdO8SQef",
        "outputId": "c6386e5f-315d-48ff-ac84-a44300b0cd8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['cats','cacti','radii','feet','speeches', \"runner\", 'bleed']\n",
        "for word in words:\n",
        "    print(word+' ----> '+lemmatizer.lemmatize(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRwzcAK6SVet",
        "outputId": "e25b5c55-f4b4-45b3-f5b6-067a66d8007d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cats ----> cat\n",
            "cacti ----> cactus\n",
            "radii ----> radius\n",
            "feet ----> foot\n",
            "speeches ----> speech\n",
            "runner ----> runner\n",
            "bleed ----> bleed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemmatizer.lemmatize(\"meeting\", \"n\"))\n",
        "print(lemmatizer.lemmatize(\"meeting\",'v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3Z26EEPTjnM",
        "outputId": "01ea34e3-aea9-4511-cd43-17a0d2c61a65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "meeting\n",
            "meet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "sentence = 'He was running and eating at the same time. He has a bad habit of swimming after playing long hours in the Sun.'\n",
        "punctuations= '?:!.,;'\n",
        "sentence_words = nltk.word_tokenize(sentence)\n",
        "for word in sentence_words:\n",
        "    if word in punctuations:\n",
        "        sentence_words.remove(word)\n",
        "sentence_words\n",
        "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
        "for word in sentence_words:\n",
        "    print (\"{0:20}{1:20}\".format(word,lemmatizer.lemmatize(word)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5xou3OcTuG8",
        "outputId": "7dcba7e8-8e3c-4729-cf28-60ee3d1bc05b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word                Lemma               \n",
            "He                  He                  \n",
            "was                 wa                  \n",
            "running             running             \n",
            "and                 and                 \n",
            "eating              eating              \n",
            "at                  at                  \n",
            "the                 the                 \n",
            "same                same                \n",
            "time                time                \n",
            "He                  He                  \n",
            "has                 ha                  \n",
            "a                   a                   \n",
            "bad                 bad                 \n",
            "habit               habit               \n",
            "of                  of                  \n",
            "swimming            swimming            \n",
            "after               after               \n",
            "playing             playing             \n",
            "long                long                \n",
            "hours               hour                \n",
            "in                  in                  \n",
            "the                 the                 \n",
            "Sun                 Sun                 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in sentence_words:\n",
        "    print (\"{0:20}{1:20}\".format(word,lemmatizer.lemmatize(word, pos='v')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZrytZvBUvAr",
        "outputId": "07e35fdc-265c-4a06-946b-dc2591b17a23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "He                  He                  \n",
            "was                 be                  \n",
            "running             run                 \n",
            "and                 and                 \n",
            "eating              eat                 \n",
            "at                  at                  \n",
            "the                 the                 \n",
            "same                same                \n",
            "time                time                \n",
            "He                  He                  \n",
            "has                 have                \n",
            "a                   a                   \n",
            "bad                 bad                 \n",
            "habit               habit               \n",
            "of                  of                  \n",
            "swimming            swim                \n",
            "after               after               \n",
            "playing             play                \n",
            "long                long                \n",
            "hours               hours               \n",
            "in                  in                  \n",
            "the                 the                 \n",
            "Sun                 Sun                 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['is', 'was', 'be', 'been', 'are', 'were', 'being']\n",
        "for word in words:\n",
        "    print(word+' ----> '+lemmatizer.lemmatize(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cm8PArvDVNFQ",
        "outputId": "aaaad494-1460-4080-9dff-3e804f462082"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "is ----> is\n",
            "was ----> wa\n",
            "be ----> be\n",
            "been ----> been\n",
            "are ----> are\n",
            "were ----> were\n",
            "being ----> being\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['is', 'was', 'be', 'been', 'are', 'were', 'being']\n",
        "for word in words:\n",
        "    print(word.title()+' ----> '+lemmatizer.lemmatize(word, pos='v').title())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaheXAYAVrz_",
        "outputId": "ad688ee4-0051-46cb-a2bb-a279a1e113c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is ----> Be\n",
            "Was ----> Be\n",
            "Be ----> Be\n",
            "Been ----> Be\n",
            "Are ----> Be\n",
            "Were ----> Be\n",
            "Being ----> Be\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['feet','radii','men', 'childern','carpenter','fighter']\n",
        "for word in words:\n",
        "    print(word.title()+' ----> '+lemmatizer.lemmatize(word, pos='n').title())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7V60RjhnVwWt",
        "outputId": "378ed21a-f1f1-4c50-8f32-5d9e50f0a246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feet ----> Foot\n",
            "Radii ----> Radius\n",
            "Men ----> Men\n",
            "Childern ----> Childern\n",
            "Carpenter ----> Carpenter\n",
            "Fighter ----> Fighter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"الجري\",\"تجري\",\"يجرون\",\"يجري\"]\n",
        "for word in words:\n",
        "    print(word+' ----> '+ps.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBMNBgFdWZEc",
        "outputId": "11772a42-7896-4280-fac3-cb84e0f203b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "الجري ----> الجري\n",
            "تجري ----> تجري\n",
            "يجرون ----> يجرون\n",
            "يجري ----> يجري\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XhTNznVwXIPd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}